{"cells":[{"cell_type":"markdown","metadata":{"id":"cXJ-BOCKWBDE"},"source":["<h1> RBPN-VSR </h1>\n","\n","**Index:**\n","\n","1) Imports\n","2) Hyperparameters\n","3) Helper Functions\n","4) Base Networks\n","5) Derivative Networks\n","6) DBPNS and RBPN\n","7) Image Processing Helper Functions\n","8) Dataset and Dataloader\n","9) (Optional) Visualisation of Dataset\n","10) Model and Training Loop\n","11) Upscale function\n","12) Statistics\n"]},{"cell_type":"markdown","metadata":{"id":"nMZpaFmsWBDF"},"source":["TODO:\n","Problem with bottlenecked (Ignore for now)"]},{"cell_type":"markdown","metadata":{"id":"T-OADGJuWBDG"},"source":["Imports:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T13:32:14.898562Z","iopub.status.busy":"2024-03-10T13:32:14.897734Z","iopub.status.idle":"2024-03-10T13:32:14.905097Z","shell.execute_reply":"2024-03-10T13:32:14.904086Z","shell.execute_reply.started":"2024-03-10T13:32:14.898531Z"},"id":"mMLTA7HOWBDG","trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","import torchvision.transforms as transforms\n","import torch.utils.data as data\n","from PIL import Image, ImageOps\n","import torch.optim as optim\n","\n","import os\n","from os.path import join\n","from math import log2\n","import random\n","from torchvision.models import vgg19, VGG19_Weights\n","\n","print(\"Succesfully loaded imports\")"]},{"cell_type":"markdown","metadata":{"id":"Hh2vHNZWWBDG"},"source":["Hyperparameters:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T13:32:14.907338Z","iopub.status.busy":"2024-03-10T13:32:14.906936Z","iopub.status.idle":"2024-03-10T13:32:14.916133Z","shell.execute_reply":"2024-03-10T13:32:14.915167Z","shell.execute_reply.started":"2024-03-10T13:32:14.907306Z"},"id":"eWrIlPsAWBDH","trusted":true},"outputs":[],"source":["epochs = 2\n","lr = 1e-4\n","base_filter = 256\n","feat = 64\n","n_resblock = 5\n","nFrames = 7\n","upscale_factor = 2\n","patch_size = 64 #0 to use original frame size\n","batch_size = 5\n","test_batch_size = 1\n","snapshot = 1\n","record = 50\n","\n","residual = True\n","future_frame = False #Upscale function assumes the training has been done using future_frame=True. Will modify it later...\n","data_augmentation = True\n","useBottlenecked = False\n","useInitialLayer = False\n","usePixelShuffle = True\n","pretrained = False\n","testing = False\n","doScaleFlow = False\n","\n","data_dir_T = \"/kaggle/input/vsr-7-frame-videos-dataset/vimeo_test_clean/S_Train\" #1 folder above where the all the subfolders containg 7-image sequences are kept\n","file_list_T = \"../S_Train_List.txt\" #the txt file containing the list of subfolders\n","data_dir_V = \"/kaggle/input/vsr-7-frame-videos-dataset/vimeo_test_clean/S_Test\"\n","file_list_V = \"../S_Test_List.txt\"\n","save_folder = \"./\"\n","logfile_name = \"VSR_P_log.txt\"\n","param_filename = \"VSR_P_\"\n","pretrained_params = \"./VSR_P_1.pth\"\n","\n","epoch_loss_hist = []\n","iter_loss_hist = []"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T13:32:14.917873Z","iopub.status.busy":"2024-03-10T13:32:14.917325Z","iopub.status.idle":"2024-03-10T13:32:14.927306Z","shell.execute_reply":"2024-03-10T13:32:14.92639Z","shell.execute_reply.started":"2024-03-10T13:32:14.917843Z"},"trusted":true},"outputs":[],"source":["print(f\"original_lr:{lr}, batch_size:{batch_size},\\\n","               upscale_factor:{upscale_factor}, future_frame:{future_frame}, nFrames:{nFrames}, residual:{residual}, base_filter:{base_filter},\\\n","                feat:{feat}, pretrained:{pretrained}, pretrained_params:{pretrained_params if pretrained else None}, n_resblock:{n_resblock},\\\n","                data_augmentation:{data_augmentation}, useBottlenecked:{useBottlenecked}, useInitialLayer:{useInitialLayer}, usePixelShuffle:{usePixelShuffle}, doScaleFlow:{doScaleFlow}\")"]},{"cell_type":"markdown","metadata":{"id":"zjsSyAYZWBDI"},"source":["Helper Functions:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T13:32:14.929176Z","iopub.status.busy":"2024-03-10T13:32:14.928857Z","iopub.status.idle":"2024-03-10T13:32:14.943663Z","shell.execute_reply":"2024-03-10T13:32:14.942847Z","shell.execute_reply.started":"2024-03-10T13:32:14.929153Z"},"id":"LTi6TA71WBDI","trusted":true},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","class vggL(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.vgg = vgg19(weights=VGG19_Weights.DEFAULT).features[:25].eval().to(device)\n","        self.MSE = nn.MSELoss()\n","\n","    def forward(self, first, second):\n","        vgg_first = self.vgg(first)\n","        vgg_second = self.vgg(second)\n","        perceptual_loss = self.MSE(vgg_first, vgg_second)\n","        return perceptual_loss\n","\n","def GetLayer(nameOfLayer, *num_params):\n","    if nameOfLayer=='batch':\n","        return nn.BatchNorm2d(*num_params)\n","    elif nameOfLayer=='instance':\n","        return nn.InstanceNorm2d(*num_params)\n","    elif nameOfLayer=='relu':\n","        return nn.ReLU(True)\n","    elif nameOfLayer=='lrelu':\n","        return nn.LeakyReLU(0.2, True)\n","    elif nameOfLayer=='prelu':\n","        return nn.PReLU(*num_params) #Otherwise 1\n","    elif nameOfLayer=='tanh':\n","        return nn.Tanh()\n","    elif nameOfLayer=='sigmoid':\n","        return nn.Sigmoid()\n","    else:\n","        return nn.Identity()\n","\n","ToResizedPIL = transforms.Compose([\n","    transforms.Resize((256, 448), interpolation=transforms.InterpolationMode.BICUBIC),\n","    transforms.ToPILImage()\n","])\n","\n","def PrintNetwork(net):\n","    num_params = 0\n","    for param in net.parameters():\n","        num_params += param.numel()\n","    print(net)\n","    print(f'Total number of parameters: {num_params}')\n","\n","def Checkpoint(model, save_folder, curr_epoch, loss, file_name=\"\", paramLogFilename=\"\"):\n","    torch.save(model.state_dict(), save_folder+file_name+f\"{curr_epoch}.pth\")\n","    print(f\"Checkpoint saved to {file_name}{curr_epoch}.pth\")\n","    log = open(save_folder+paramLogFilename, 'a')\n","    log.write(f\"Version:{file_name}{curr_epoch}.pth, Epoch:{curr_epoch}/{epochs}, original_lr:{lr}, batch_size:{batch_size}, gen_loss:{loss}, \\\n","               upscale_factor:{upscale_factor}, future_frame:{future_frame}, nFrames:{nFrames}, residual:{residual}, base_filter:{base_filter},\\\n","                feat:{feat}, pretrained:{pretrained}, pretrained_params:{pretrained_params if pretrained else None}, n_resblock:{n_resblock},\\\n","                data_augmentation:{data_augmentation}, useBottlenecked:{useBottlenecked}, useInitialLayer:{useInitialLayer}, usePixelShuffle:{usePixelShuffle}, doScaleFlow:{doScaleFlow}\")\n","    log.close()\n","    print(f\"Checkpoint details updated to {save_folder+paramLogFilename}\")\n","\n","def custom_stretch(img_mat):\n","    img_mat_c = img_mat.astype(np.float32)\n","    max = np.max(img_mat)\n","    min = np.min(img_mat)\n","    return np.floor((255*(img_mat_c-min)/(max-min))).astype(np.uint8)\n"]},{"cell_type":"markdown","metadata":{"id":"ENDxSvmBWBDI"},"source":["Base Networks:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T13:32:15.029152Z","iopub.status.busy":"2024-03-10T13:32:15.02886Z","iopub.status.idle":"2024-03-10T13:32:15.042494Z","shell.execute_reply":"2024-03-10T13:32:15.041597Z","shell.execute_reply.started":"2024-03-10T13:32:15.029129Z"},"id":"n9y48XauWBDK","trusted":true},"outputs":[],"source":["#Conv2d -> BN -> Activation ->\n","#[B, input_size, H, W] -> [B, output_size, H, W] (Assuming default KSP)\n","class ConvBlock(nn.Module):\n","    def __init__(self, input_size, output_size, kernel_size=3, stride=1, padding=1, bias=True, activation='prelu', norm=None):\n","        super(ConvBlock, self).__init__()\n","\n","        self.conv = nn.Conv2d(input_size, output_size, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n","        self.bn = GetLayer(norm, output_size)\n","        self.act = GetLayer(activation, output_size)\n","\n","    def forward(self, x):\n","        out = self.bn(self.conv(x))\n","        return self.act(out)\n","\n","#ConvTranspose -> BN -> Activation ->\n","#[B, input_size, H, W] -> [B, output_size, 2*H, 2*W] (Assuming default KSP)\n","class DeconvBlock(nn.Module):\n","    def __init__(self, input_size, output_size, kernel_size=4, stride=2, padding=1, bias=True, activation='prelu', norm=None):\n","        super(DeconvBlock, self).__init__()\n","\n","        self.deconv = nn.ConvTranspose2d(input_size, output_size, kernel_size, stride, padding, bias=bias)\n","        self.bn = GetLayer(norm, output_size)\n","        self.act = GetLayer(activation, output_size)\n","\n","    def forward(self, x):\n","        out = self.bn(self.deconv(x))\n","        return self.act(out)\n","\n","\n","#Essentially a series of alternating ConvBlocks that take n_feat input channels and outputs n_feat output channels\n","#with no change in input image size and a pixel shuffle layer which takes the factor 4 in the received input channels and splits the\n","#extra pixels in half between width and height, doubling both dimensions and thus doubling the input size as output.\n","#These 2 layers are repeated at most k times where scale = 2**k, so if scale=8, the final result of these string of pairs of layers\n","#is an image with same number of channels as you had inputted but with 8x the size due to there being 3 x2 pairs of these layers.\n","#Optional batch norm after each pair.\n","\n","# (Conv -> PS -> BN) -> (Conv -> PS -> BN) -> ... -> Activation ->\n","#[B, n_feat, H, W] -> [B, n_feat, scale*H, scale*W]\n","class Upsampler(nn.Module):\n","    def __init__(self, scale, n_feat, bn=False, activation='prelu', bias=True):\n","        super(Upsampler, self).__init__()\n","\n","        modules = []\n","        for _ in range(int(log2(scale))):\n","            modules.append(ConvBlock(n_feat, 4 * n_feat, 3, 1, 1, bias, activation=None, norm=None))\n","            modules.append(nn.PixelShuffle(2))\n","            if bn:\n","                modules.append(nn.BatchNorm2d(n_feat))\n","        self.up = nn.Sequential(*modules)\n","        self.act = GetLayer(activation, n_feat)\n","\n","    def forward(self, x):\n","        out = self.up(x)\n","        out = self.act(out)\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"YBVm9X_PWBDK"},"source":["Derivative Networks:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T13:32:15.04468Z","iopub.status.busy":"2024-03-10T13:32:15.044349Z","iopub.status.idle":"2024-03-10T13:32:15.06562Z","shell.execute_reply":"2024-03-10T13:32:15.064745Z","shell.execute_reply.started":"2024-03-10T13:32:15.044651Z"},"id":"MgSJKZEvWBDK","trusted":true},"outputs":[],"source":["#None of the Conv2d layers change the size of the input images using default K,S,P\n","#[B, num_filter, H, W] -> [B, num_filter, H, W] (Assuming default KSP)\n","class ResnetBlock(nn.Module):\n","    def __init__(self, num_filter, kernel_size=3, stride=1, padding=1, bias=True, activation='prelu', norm='batch', useBottlenecked=useBottlenecked):\n","        super(ResnetBlock, self).__init__()\n","\n","        self.bottleneck_in = nn.Identity()\n","        self.bottleneck_out = nn.Identity()\n","        if useBottlenecked:\n","            print(\"Changing filter sizes:\")\n","            orig_filter = num_filter\n","            num_filter = num_filter//2\n","            print(f\"Orig_filter:{orig_filter}, num_filter:{num_filter}\")\n","            self.bottleneck_in = nn.Conv2d(orig_filter, num_filter, kernel_size=3, stride=1, padding=1, bias=True)\n","            self.bottleneck_out = nn.Conv2d(num_filter, orig_filter, kernel_size=3, stride=1, padding=1, bias=True)\n","\n","        self.conv1 = nn.Conv2d(num_filter, num_filter, kernel_size, stride, padding, bias=bias)\n","        self.conv2 = nn.Conv2d(num_filter, num_filter, kernel_size, stride, padding, bias=bias)\n","        self.bn = GetLayer(norm, num_filter)\n","        self.act = GetLayer(activation, num_filter)\n","\n","    def forward(self, x):\n","        x = self.bottleneck_in(x)\n","        residual = x\n","        out = self.bn(self.conv1(x))\n","        out = self.act(out)\n","        out = self.bn(self.conv2(out))\n","        out = torch.add(out, residual)\n","        out = self.bottleneck_out(out)\n","        out = self.act(out)\n","        return out\n","\n","#[B, num_filter, H, W] -> [B, num_filter, stride*H, stride*W]\n","#If you want to affect size of output, change the stride to be 2**n\n","class UpBlock(nn.Module):\n","    def __init__(self, num_filter, kernel_size=8, stride=4, padding=2, num_stages=1, activation='prelu', norm=None, useInitialLayer=useInitialLayer, usePS=usePixelShuffle):\n","        super(UpBlock, self).__init__()\n","\n","        self.initial_layer = nn.Identity()\n","        if useInitialLayer:\n","            self.initial_layer = ConvBlock(num_stages*num_filter, num_filter, kernel_size=1, stride=1, padding=0, activation=activation, norm=norm)\n","\n","        self.up1 = Upsampler(stride, num_filter, activation=activation) if usePS else DeconvBlock(num_filter, num_filter, kernel_size, stride, padding, activation=activation, norm=norm)\n","        self.up2 = ConvBlock(num_filter, num_filter, kernel_size, stride, padding, activation, norm=None)\n","        self.up3 = Upsampler(stride, num_filter, activation=activation) if usePS else DeconvBlock(num_filter, num_filter, kernel_size, stride, padding, activation=activation, norm='batch')\n","\n","    def forward(self, x):\n","        x = self.initial_layer(x)\n","        h0 = self.up1(x)\n","        l0 = self.up2(h0)\n","        h1 = self.up3(l0 - x)\n","        return h1 + h0\n","\n","#Scale=Stride\n","#[B, num_filter, H, W] -> [B, num_filter, H/4, W/4] (Assuming default KSP)\n","class DownBlock(nn.Module):\n","    def __init__(self, num_filter, kernel_size=8, stride=4, padding=2, num_stages=1, activation='prelu', norm=None, useInitialLayer=useInitialLayer, usePS=usePixelShuffle):\n","        super(DownBlock, self).__init__()\n","\n","        self.initial_layer = nn.Identity()\n","        if useInitialLayer:\n","            self.initial_layer = ConvBlock(num_stages*num_filter, num_filter, kernel_size=1, stride=1, padding=0, activation=activation, norm=norm)\n","\n","        self.down1 = ConvBlock(num_filter, num_filter, kernel_size, stride, padding, activation=activation, norm=norm)\n","        self.down2 = Upsampler(stride, num_filter, activation=activation) if usePS else DeconvBlock(num_filter, num_filter, kernel_size, stride, padding, activation=activation, norm=norm)\n","        self.down3 = ConvBlock(num_filter, num_filter, kernel_size, stride, padding, activation=activation, norm=norm)\n","\n","    def forward(self, x):\n","        x = self.initial_layer(x)\n","        l0 = self.down1(x)\n","        h0 = self.down2(l0)\n","        l1 = self.down3(h0-x)\n","        return l0 + l1"]},{"cell_type":"markdown","metadata":{"id":"SBTcgAplWBDK"},"source":["DBPNS and RBPN:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T13:32:15.06716Z","iopub.status.busy":"2024-03-10T13:32:15.066864Z","iopub.status.idle":"2024-03-10T13:32:15.092198Z","shell.execute_reply":"2024-03-10T13:32:15.091205Z","shell.execute_reply.started":"2024-03-10T13:32:15.067138Z"},"id":"XLxHN8EtWBDK","trusted":true},"outputs":[],"source":["#Essentially a series of upsampling and downsampling layers using the UpBlock and DownBlock respectively. Num_stages might be hardcoded to 3. I can't tell by\n","#how the number of pairs of these blocks are hardcoded and not variable... After looking at the supermost caller of this function, num_stages is indeed inputted as 3\n","#Output has (feat) number of out channels and outputs 2x the input dimensions\n","#The convolution-related layers are manually initialized with a mathematical distribution which is the kaiming normal distribution seen below\n","#The 2 conv blocks do not affect input dimension size, only number of channels\n","\n","#Can change the num_stages so that the computation computes pairs of the blocks num_stages number of times but only concatenates the last 3 outputs...\n","class DBPNS(nn.Module):\n","    def __init__(self, base_filter, feat, num_stages, scale_factor):\n","        super(DBPNS, self).__init__()\n","\n","        #Mostly defined for scale_factors of 2,4,8, ..., 2**n\n","        stride = scale_factor\n","        kernel = stride + 4\n","        padding = 2\n","\n","        self.feat1 = ConvBlock(base_filter, feat, 1, 1, 0, activation='prelu', norm='batch')\n","        #Back-projection stages\n","        #Consider extending this to any number of num_stages other than just 3.\n","        self.up1 = UpBlock(num_filter=feat, kernel_size=kernel, stride=stride, padding=padding)\n","        self.down1 = DownBlock(num_filter=feat, kernel_size=kernel, stride=stride, padding=padding)\n","        self.up2 = UpBlock(num_filter=feat, kernel_size=kernel, stride=stride, padding=padding)\n","        self.down2 = DownBlock(num_filter=feat, kernel_size=kernel, stride=stride, padding=padding)\n","        self.up3 = UpBlock(num_filter=feat, kernel_size=kernel, stride=stride, padding=padding)\n","        #Reconstruction\n","        self.output = ConvBlock(num_stages*feat, feat, 1, 1, 0, activation=None, norm='batch') #num_stages*feat is essentially 3*feat unless I plan to modularize this\n","\n","        #Initialization of convolution and conv transpose layers using a mathematical distribution called kaiming normal distribution\n","        # for m in self.modules():\n","        #     class_names = m.__class__.__name__\n","        #     if class_names.find('Conv2d') != -1 or class_names.find('ConvTranspose2d') != -1:\n","        #         nn.init.kaiming_normal_(m.weight)\n","        #         if m.bias is not None:\n","        #             m.bias.data.zero_()\n","\n","    def forward(self, x):\n","        x = self.feat1(x)\n","\n","        h1 = self.up1(x)\n","        h2 = self.up2(self.down1(h1))\n","        h3 = self.up3(self.down2(h2))\n","        #Think of each h_i being in the shape [B,C,H,W]=[B,feat,H,W] and torch.cat along dimension 1 adds feat 3 times => [B, 3*feat, H, W]\n","        #This furthers my suspicion that num_stages is actually hardcoded as 3 otherwise this wouldn't work since self.out would take num_stages*feat =/= 3*feat input channels\n","        x = self.output(torch.cat((h3, h2, h1),1))\n","        return x\n","\n","\n","#Output has num_channels output channels with x scale factor size\n","class RBPN(nn.Module):\n","    def __init__(self, num_channels, base_filter, feat, num_stages, n_resblock, nFrames, scale_factor):\n","        super(RBPN, self).__init__()\n","        #base_filter=256\n","        #feat=64\n","        self.nFrames = nFrames\n","\n","        #Mostly defined for scale_factors of 2,4,8, ..., 2**n\n","        stride = scale_factor\n","        kernel = stride + 4\n","        padding = 2\n","\n","        #Initial Feature Extraction (These 2 layers are not consecutive during forward pass.\n","        #8 input channels for feat1 because input frame has 3 channels, neighbouring\n","\t\t#frame has 3 channels and flow between these frames has 2 channels for (x,y) displacement vector)\n","        self.feat0 = ConvBlock(num_channels, base_filter, 3, 1, 1, activation='prelu', norm='batch')\n","        self.feat1 = ConvBlock(8, base_filter, 3, 1, 1, activation='prelu', norm=None)\n","\n","        # --- START OF ENCODER --- #\n","        ###DBPNS (Output is x scale_factor) (SISR Block)\n","        self.DBPN = DBPNS(base_filter, feat, num_stages, scale_factor)\n","\n","        #Res-Block1 (Resnet MISR Block) (Output is times scale_factor)\n","        modules_body1 = [ResnetBlock(base_filter, kernel_size=3, stride=1, padding=1, bias=True, activation='prelu', norm='batch', useBottlenecked=useBottlenecked) for _ in range(n_resblock)]\n","        modules_body1.append(DeconvBlock(base_filter, feat, kernel, stride, padding, activation='prelu', norm='batch')) # x scale_factor\n","        self.res_feat1 = nn.Sequential(*modules_body1)\n","\n","        #Res-Block2 (String of residual blocks)(Output_size = Input_size and number of channels is constant)(As per Fig 4. a) in the original paper)\n","        modules_body2 = [ResnetBlock(feat, kernel_size=3, stride=1, padding=1, bias=True, activation='prelu', norm='batch', useBottlenecked=useBottlenecked) for _ in range(n_resblock)]\n","        modules_body2.append(ConvBlock(feat, feat, 3, 1, 1, activation='prelu', norm=None)) #No change to sizes and channels\n","        self.res_feat2 = nn.Sequential(*modules_body2)\n","\n","        # --- END OF ENCODER --- #\n","\n","        #Res-Block3 (Decoder block as per diagram 4 b) of the original paper)(Output size is input size divided by scale_factor)(Output channels = base_filter always)\n","        modules_body3 = [ResnetBlock(feat, kernel_size=3, stride=1, padding=1, bias=True, activation='prelu', norm='batch', useBottlenecked=useBottlenecked) for _ in range(n_resblock)]\n","        modules_body3.append(ConvBlock(feat, base_filter, kernel, stride, padding, activation='prelu', norm='batch')) #Divides size by scale_factor\n","        self.res_feat3 = nn.Sequential(*modules_body3)\n","\n","        #Reconstruction\n","        self.output = ConvBlock((nFrames-1)*feat, num_channels, 3, 1, 1, activation=None, norm=None) #Doesn't change size of input. nFrames is the\n","                                                                                                     #interval of frames around the initial input frame\n","                                                                                                     #to consider for flow computation and MISR. nFrames<=7\n","\n","        #Initialization of Convolution-related layers.\n","        for m in self.modules():\n","            class_names = m.__class__.__name__\n","            if class_names.find('Conv2d') != -1 or class_names.find('ConvTranspose2d') != -1:\n","                nn.init.kaiming_normal_(m.weight)\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","\n","    def forward(self, x, neighbour, flow):\n","        feat_frame=[]\n","\t\t    #Concat\n","        for j in range(len(neighbour)):\n","            feat_frame.append(self.feat1(torch.cat((x, neighbour[j], flow[j]),1))) #8 input channels\n","        ### initial feature extraction\n","        feat_input = self.feat0(x)\n","\n","        ####Projection\n","        Ht = []\n","        for j in range(len(neighbour)):\n","            h0 = self.DBPN(feat_input) #SISR using DBPNS, output is x scale_factor\n","            h1 = self.res_feat1(feat_frame[j]) #MISR using Resnet, output is x scale_factor\n","            e = h0-h1 #SISR - MISR\n","            e = self.res_feat2(e) #Does not affect input size.\n","            h = h0+e #As per diagram 4 a) of original paper\n","            Ht.append(h) #All outputs of the projection are finally needed to be concatenated to get final output. Each h has (feat) output channels\n","            feat_input = self.res_feat3(h) #Input to the next projection module updated - This is the recurrent part of the network.\n","        ####Reconstruction\n","        out = torch.cat(Ht,1) #Concatenating along channels\n","        output = self.output(out) #No change in size. Output has num_channel output channels. Convolution layer as per stated architecture.\n","        return output"]},{"cell_type":"markdown","metadata":{"id":"0ROswdOuWBDL"},"source":["Image Processing Helper Functions:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T13:32:15.094257Z","iopub.status.busy":"2024-03-10T13:32:15.093904Z","iopub.status.idle":"2024-03-10T13:32:15.113966Z","shell.execute_reply":"2024-03-10T13:32:15.113225Z","shell.execute_reply.started":"2024-03-10T13:32:15.094234Z"},"id":"6dHoAAg_WBDL","trusted":true},"outputs":[],"source":["def modcrop(img, modulo):\n","    (ih, iw) = img.size\n","    ih = ih - (ih%modulo)\n","    iw = iw - (iw%modulo)\n","    img = img.crop((0, 0, ih, iw))\n","    return img\n","\n","def is_image_file(filename):\n","    return any(filename.endswith(extension) for extension in [\".png\", \".jpg\", \".jpeg\"])\n","\n","def rescale_flow(x,min_range,max_range):\n","    max_val = np.max(x)\n","    min_val = np.min(x)\n","    l = x\n","    try:\n","        l = ((max_range-min_range)/(max_val-min_val))*(x-min_val)+min_range\n","    except RuntimeWarning:\n","        l = x\n","    finally:\n","        return l\n","\n","def get_flow(im1, im2, returnTensor=True, rescale=False):\n","    im1_mat = np.array(im1.convert('L'), dtype=np.uint8)\n","    im2_mat = np.array(im2.convert('L'), dtype=np.uint8)\n","\n","    h,w = im1_mat.shape\n","    im1_mat = im1_mat.reshape(h, w, 1)\n","    im2_mat = im2_mat.reshape(h, w, 1)\n","\n","    flow = cv2.calcOpticalFlowFarneback(im1_mat, im2_mat, None, 0.5, 3, 15, 3, 5, 1.1, 0)\n","    if rescale:\n","        flow = rescale_flow(flow, 0, 1)\n","    flow = flow.reshape(2,h,w)\n","    if returnTensor:\n","        flow = torch.from_numpy(flow)\n","\n","    return flow\n","\n","def rescale_img(img_in, scale):\n","    size_in = img_in.size\n","    new_size_in = tuple([int(x * scale) for x in size_in])\n","    img_in = img_in.resize(new_size_in, resample=Image.BICUBIC)\n","    return img_in\n","\n","\n","#Obtains a patch of an image with bounding box of size patch_size with upper left corner at (ix,iy). Returns PIL Image\n","def get_patch(img_in, img_tar, img_nn, patch_size, scale, ix=-1, iy=-1):\n","    (ih, iw) = img_in.size\n","\n","    patch_mult = scale\n","    tp = patch_mult * patch_size\n","    ip = tp // scale\n","    #ip = patch_size if scale = patch_mult = integer, which is the case for default paramaters...\n","\n","    if ix == -1:\n","        ix = torch.randint(0, iw - ip + 1,(1,)).item()\n","    if iy == -1:\n","        iy = torch.randint(0, ih - ip + 1,(1,)).item()\n","\n","    (tx, ty) = (scale * ix, scale * iy)\n","\n","    img_in = img_in.crop((iy,ix,iy + ip, ix + ip))\n","    img_tar = img_tar.crop((ty,tx,ty + tp, tx + tp))\n","    img_nn = [j.crop((iy,ix,iy + ip, ix + ip)) for j in img_nn]\n","\n","    info_patch = {\n","        'ix': ix, 'iy': iy, 'ip': ip, 'tx': tx, 'ty': ty, 'tp': tp}\n","\n","    return img_in, img_tar, img_nn, info_patch\n","\n","def augment(img_in, img_tar, img_nn, flip_h=True, rot=True):\n","    info_aug = {'flip_h': False, 'flip_v': False, 'trans': False} #used for seeing what actually happened to a particular batch of images. Debugging\n","\n","    if random.random() < 0.5 and flip_h:\n","        img_in = ImageOps.flip(img_in)\n","        img_tar = ImageOps.flip(img_tar)\n","        img_nn = [ImageOps.flip(j) for j in img_nn]\n","        info_aug['flip_h'] = True\n","\n","    if rot:\n","        if random.random() < 0.5:\n","            img_in = ImageOps.mirror(img_in)\n","            img_tar = ImageOps.mirror(img_tar)\n","            img_nn = [ImageOps.mirror(j) for j in img_nn]\n","            info_aug['flip_v'] = True\n","        if random.random() < 0.5:\n","            img_in = img_in.rotate(180)\n","            img_tar = img_tar.rotate(180)\n","            img_nn = [j.rotate(180) for j in img_nn]\n","            info_aug['trans'] = True\n","\n","    return img_in, img_tar, img_nn, info_aug"]},{"cell_type":"markdown","metadata":{"id":"WmW4XGCmWBDM"},"source":["Dataset and Dataloaders:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T13:32:15.115585Z","iopub.status.busy":"2024-03-10T13:32:15.115257Z","iopub.status.idle":"2024-03-10T13:32:15.189683Z","shell.execute_reply":"2024-03-10T13:32:15.1868Z","shell.execute_reply.started":"2024-03-10T13:32:15.115563Z"},"id":"0J1Zmf2JWBDM","outputId":"8ca19174-bc24-42a9-f063-31403ba10f54","trusted":true},"outputs":[],"source":["#nFrames = 7 default (Can be reduced)\n","#file path is actually the path to the folder containing the 7 image sequence w.r.t a cwd\n","def load_img(filepath, nFrames, scale, useModcrop=True):\n","    seq = [i for i in range(1, nFrames)] #seq=[1,2,3,4,5,6]\n","\n","    target = Image.open(join(filepath,'im'+str(nFrames)+'.png')).convert('RGB')\n","    if useModcrop:\n","        target = modcrop(target, scale) #scale is default 4, target is just the 7th frame modcropped\n","\n","    input = target.resize((int(target.size[0]/scale),int(target.size[1]/scale)), Image.BICUBIC)\n","    neighbour = [modcrop(Image.open(filepath+'/im'+str(j)+'.png').convert('RGB'), scale).resize((int(target.size[0]/scale),int(target.size[1]/scale)), Image.BICUBIC) for j in reversed(seq)]\n","\n","    #Neighbours is a list of PIL images in the order of frames 6,5,...,1 which has 1) been modcropped 2) then resized to same dimensions as input, in that order.\n","    return target, input, neighbour\n","\n","#does what load_img does but for the 4th frame, not the 7th\n","def load_img_future(filepath, nFrames, scale, useModcrop=True):\n","    tt = int(nFrames/2)\n","    target = Image.open(join(filepath,'im'+str(nFrames)+'.png')).convert('RGB')\n","    if useModcrop:\n","        target = modcrop(target, scale)\n","    inp = target.resize((int(target.size[0]/scale),int(target.size[1]/scale)), Image.BICUBIC)\n","    neighbour = []\n","    seq = [x for x in range(4-tt,5+tt) if x!=4]\n","\n","    for j in seq:\n","        neighbour.append(modcrop(Image.open(filepath+'/im'+str(j)+'.png').convert('RGB'), scale).resize((int(target.size[0]/scale),int(target.size[1]/scale)), Image.BICUBIC))\n","    return target, inp, neighbour\n","\n","class VideoSequenceSet(data.Dataset):\n","    def __init__(self, image_dir, image_list, nFrames=7, scale_factor=2, patch_size=64, useFuture=True, doAugmentation = True, train=True, useModcrop=True):\n","        super(VideoSequenceSet, self).__init__()\n","        listOfVideos = [line.rstrip() for line in open(join(image_dir,image_list))]\n","        self.pathToVideos = [join(image_dir,x) for x in listOfVideos]\n","\n","        self.num_frames = nFrames\n","        self.upscale_factor = scale_factor\n","        self.patch_size = patch_size\n","\n","        self.useFuture = useFuture\n","        self.train = train\n","        self.doAugmentation = doAugmentation\n","        self.useModcrop = useModcrop\n","\n","    def __getitem__(self, index):\n","        if self.useFuture:\n","            target, inp, neighbour = load_img_future(self.pathToVideos[index], self.num_frames, self.upscale_factor, useModcrop=self.useModcrop)\n","        else:\n","            target, inp, neighbour = load_img(self.pathToVideos[index], self.num_frames, self.upscale_factor, useModcrop=self.useModcrop)\n","\n","        if self.train:\n","            if self.patch_size != 0:\n","                inp, target, neighbour, _ = get_patch(inp,target,neighbour,self.patch_size, self.upscale_factor)\n","\n","            if self.doAugmentation: #default True\n","                inp, target, neighbour, _ = augment(inp, target, neighbour, flip_h=True, rot=True)\n","\n","        flow_list = [get_flow(inp,j, returnTensor=True, rescale=doScaleFlow) for j in neighbour]\n","        bicubic = rescale_img(inp, self.upscale_factor)\n","\n","        T = transforms.ToTensor()\n","        target = T(target)\n","        inp = T(inp)\n","        bicubic = T(bicubic)\n","        neighbour = [T(j) for j in neighbour]\n","\n","        return inp, target, neighbour, flow_list, bicubic\n","\n","    def __len__(self):\n","        return len(self.pathToVideos)\n","\n","train_set = VideoSequenceSet(data_dir_T, file_list_T, nFrames=nFrames, scale_factor=upscale_factor, patch_size=patch_size, \\\n","                             useFuture=future_frame, train=True, doAugmentation=data_augmentation, useModcrop=True)\n","val_set = VideoSequenceSet(data_dir_V, file_list_V, nFrames=nFrames, scale_factor=upscale_factor, patch_size=0, \\\n","                             useFuture=future_frame, train=False, doAugmentation=False, useModcrop=False)\n","\n","videoT_loader = data.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n","videoV_loader = data.DataLoader(dataset=val_set, batch_size=test_batch_size, shuffle=True, num_workers=2)"]},{"cell_type":"markdown","metadata":{"id":"wyboav3FWBDM"},"source":["(Optional) Visualisation of dataset:\n","\n","[Assumes that future_frame=True]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T13:32:15.191406Z","iopub.status.busy":"2024-03-10T13:32:15.191065Z","iopub.status.idle":"2024-03-10T13:32:19.51625Z","shell.execute_reply":"2024-03-10T13:32:19.515173Z","shell.execute_reply.started":"2024-03-10T13:32:15.191371Z"},"id":"n6PG6BDOWBDM","trusted":true},"outputs":[],"source":["for idx, x in enumerate(videoV_loader):\n","    if idx==0:\n","        inp,t,n,fl,b = x\n","\n","        fig, axs = plt.subplots(1,3, figsize=(16,4))\n","        axs[0].imshow(inp[0].permute(1,2,0).numpy())\n","        axs[0].set_title(\"Input\")\n","        axs[1].imshow(t[0].permute(1,2,0).numpy())\n","        axs[1].set_title(\"Target\")\n","        axs[2].imshow(b[0].permute(1,2,0).numpy())\n","        axs[2].set_title(\"Bicubic\")\n","\n","        plt.show()\n","\n","        step=3\n","        fig, axs = plt.subplots(5,6, figsize=(16,9))\n","        for i in range(nFrames-1):\n","            axs[0][i].imshow(inp[0].permute(1,2,0).numpy())\n","            axs[0][i].set_title(f\"Input\")\n","\n","            axs[1][i].imshow(n[i][0].permute(1,2,0).numpy())\n","            axs[1][i].set_title(f\"n_0({i+1 if (i+1)<4 else i+2})\")\n","\n","            flow = fl[i][0].permute(1,2,0).numpy()\n","            axs[2][i].quiver(np.arange(0, flow.shape[1], step), np.arange(flow.shape[0], -1, -step), flow[::step, ::step, 0], flow[::step, ::step, 1])\n","            axs[2][i].set_title(f\"Quiver Diag for n_0({i+1 if (i+1)<4 else i+2})\")\n","\n","            axs[3][i].imshow(custom_stretch(fl[i][0][0].numpy()), cmap='gray')\n","            axs[3][i].set_title(f\"Flow(x) for n_0({i+1 if (i+1)<4 else i+2})\")\n","\n","            axs[4][i].imshow(custom_stretch(fl[i][0][1].numpy()), cmap='gray')\n","            axs[4][i].set_title(f\"Flow(y) for n_0({i+1 if (i+1)<4 else i+2})\")\n","\n","        for i in range(5):\n","            for j in range(6):\n","                axs[i][j].axis('off')\n","        plt.show()\n","    else:\n","        break"]},{"cell_type":"markdown","metadata":{"id":"Dyf1hMBWWBDN"},"source":["Model and Training loop:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T13:32:19.518913Z","iopub.status.busy":"2024-03-10T13:32:19.518624Z","iopub.status.idle":"2024-03-10T13:32:43.626006Z","shell.execute_reply":"2024-03-10T13:32:43.625084Z","shell.execute_reply.started":"2024-03-10T13:32:19.51889Z"},"id":"z5e_OKNPWBDN","outputId":"8fd08c94-d7b3-4eae-b30e-27e4602cc515","trusted":true},"outputs":[],"source":["model = RBPN(num_channels=3, base_filter=base_filter,  feat = feat, num_stages=3, n_resblock=n_resblock, nFrames=nFrames, scale_factor=upscale_factor).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-8)\n","criterion = nn.L1Loss()\n","vgg_loss = vggL()\n","\n","PrintNetwork(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T13:39:33.471816Z","iopub.status.busy":"2024-03-10T13:39:33.470937Z","iopub.status.idle":"2024-03-10T13:39:33.489773Z","shell.execute_reply":"2024-03-10T13:39:33.488772Z","shell.execute_reply.started":"2024-03-10T13:39:33.471782Z"},"id":"mZnIRnwuWBDN","outputId":"20e0e96e-5c85-4296-c9cf-2b2146fd218d","trusted":true},"outputs":[],"source":["def show_examples(model, loader):\n","    model.eval()\n","    model = model.cpu()\n","    chosen_batch = random.randint(1, len(loader)-1)\n","    fig, axs = plt.subplots(1, 4, figsize=(14, 10))\n","\n","    for idx, batch in enumerate(loader, 1): #Batch size=8 default\n","        if chosen_batch==idx:\n","            inp, target, neigbor, flow, bicubic = batch[0].cpu(), batch[1].cpu(), [x.cpu() for x in batch[2]], [x.cpu() for x in batch[3]], batch[4].cpu()\n","            chosen = random.randint(0, len(inp)-1)\n","            with torch.no_grad():\n","                prediction = model(inp, neigbor, flow).cpu()\n","\n","            axs[0].set_axis_off()\n","            axs[0].imshow(inp[chosen].detach().permute(1, 2, 0).numpy())\n","            axs[0].set_title(\"Input\")\n","\n","            axs[1].set_axis_off()\n","            axs[1].imshow(prediction.detach().permute(0, 2, 3, 1)[0].numpy())\n","            axs[1].set_title(\"Predicted\")\n","\n","            axs[2].set_axis_off()\n","            axs[2].imshow(target[chosen].detach().permute(1, 2, 0).numpy())\n","            axs[2].set_title(\"Target\")\n","\n","            axs[3].set_axis_off()\n","            axs[3].imshow(bicubic[chosen].detach().permute(1, 2, 0).numpy())\n","            axs[3].set_title(\"Bicubic\")\n","\n","            plt.show()\n","\n","    plt.show()\n","    model.train()\n","    model = model.to(device)\n","    \n","l = len(videoT_loader)\n","def train(model, optimizer, criterion, loader):\n","    epoch_loss = 0\n","    print(\"Started a new train function call!\")\n","    model.train()\n","\n","    for idx, batch in enumerate(loader, 1): #Batch size=8 default\n","        inp, target, neigbor, flow, bicubic = batch[0].to(device), batch[1].to(device), [x.to(device) for x in batch[2]], [x.to(device) for x in batch[3]], batch[4].to(device)\n","        prediction = model(inp, neigbor, flow)\n","        if (idx+1) % (np.ceil(l/4)) == 0:\n","            print(\"Learning rate now decaying by half...\")\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] /= 2.0 #This could be implemented via lr scheduler. Learn about that\n","            print('Learning rate decay: lr={}'.format(optimizer.param_groups[0]['lr']))\n","        if residual:\n","            prediction = prediction + bicubic\n","\n","        loss = criterion(prediction, target)+vgg_loss(prediction, target)\n","        epoch_loss = loss.data\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if idx%record == 0:\n","            print(f\"Still training... Currently at idx={idx}/{l} with current loss={epoch_loss}\")\n","            iter_loss_hist.append(epoch_loss)\n","            show_examples(model, videoV_loader)\n","\n","\n","    return epoch_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T13:32:43.64734Z","iopub.status.busy":"2024-03-10T13:32:43.646987Z","iopub.status.idle":"2024-03-10T13:32:43.72911Z","shell.execute_reply":"2024-03-10T13:32:43.728315Z","shell.execute_reply.started":"2024-03-10T13:32:43.647309Z"},"id":"0CW70JbaWBDN","outputId":"72c46619-b203-4f2e-c636-a2ab21ec1358","trusted":true},"outputs":[],"source":["if pretrained:\n","    if pretrained_params==\"\" or pretrained_params is None:\n","        print(\"File path to pretrained parameters is empty...\")\n","    else:\n","        model.load_state_dict(torch.load(pretrained_params, map_location=device))\n","        print(\"Parameters successfully loaded to model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-10T13:39:42.226017Z","iopub.status.busy":"2024-03-10T13:39:42.225397Z","iopub.status.idle":"2024-03-10T13:52:10.864867Z","shell.execute_reply":"2024-03-10T13:52:10.863285Z","shell.execute_reply.started":"2024-03-10T13:39:42.225985Z"},"id":"2L4NTP0fWBDO","trusted":true},"outputs":[],"source":["if not testing:\n","    for epoch in range(epochs):\n","        epoch_loss = train(model, optimizer, criterion, videoT_loader)\n","        print(f\"Epoch:{epoch+1}/{epochs}, loss={epoch_loss}, Progress:\")\n","        if (epoch+1) % (snapshot) == 0: #Every snapshot epochs, save params.\n","            Checkpoint(model, save_folder, epoch+1, epoch_loss, param_filename, logfile_name)\n","        epoch_loss_hist.append(epoch_loss)\n","        show_examples(model, videoV_loader)\n","\n","\n","        # learning rate is decayed by a factor of 10 every half of total epochs\n","        if (epoch+1) % (np.ceil(epochs/2)) == 0:\n","            print(\"Learning rate now decaying by half...\")\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] /= 10.0 #This could be implemented via lr scheduler. Learn about that\n","            print('Learning rate decay: lr={}'.format(optimizer.param_groups[0]['lr']))"]},{"cell_type":"markdown","metadata":{"id":"qHupcik1WBDO"},"source":["Upscale function:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:39:16.984561Z","iopub.status.idle":"2024-03-10T13:39:16.984909Z","shell.execute_reply":"2024-03-10T13:39:16.98476Z","shell.execute_reply.started":"2024-03-10T13:39:16.984743Z"},"id":"_CSIt8KhWBDO","trusted":true},"outputs":[],"source":["T = transforms.ToTensor()\n","\n","def rescale_flow(x,min_range,max_range):\n","    max_val = np.max(x)\n","    min_val = np.min(x)\n","    l = x\n","    try:\n","        l = ((max_range-min_range)/(max_val-min_val))*(x-min_val)+min_range\n","    except RuntimeWarning:\n","        l = x\n","    finally:\n","        return l\n","\n","def get_flow(im1, im2, returnTensor=True, rescale=False):\n","    im1_mat = np.array(im1.convert('L'), dtype=np.uint8)\n","    im2_mat = np.array(im2.convert('L'), dtype=np.uint8)\n","\n","    h,w = im1_mat.shape\n","    im1_mat = im1_mat.reshape(h, w, 1)\n","    im2_mat = im2_mat.reshape(h, w, 1)\n","\n","    flow = cv2.calcOpticalFlowFarneback(im1_mat, im2_mat, None, 0.5, 3, 15, 3, 5, 1.1, 0)\n","    if rescale:\n","        flow = rescale_flow(flow, 0, 1)\n","    flow = flow.reshape(2,h,w)\n","    if returnTensor:\n","        flow = torch.from_numpy(flow)\n","\n","    return flow\n","\n","def keyframes_to_video(image_list, video_path, fps):\n","    image_list_numpy = [image[0].permute(1,2,0).numpy() for image in image_list]\n","    h, w, c = image_list_numpy[0].shape\n","    compiled_video = cv2.VideoWriter(video_path, fourcc = cv2.VideoWriter_fourcc('D','I','V','X'), fps = fps, frameSize = (w, h))\n","    print(image_list_numpy[0].shape)\n","    for image in image_list_numpy:\n","        compiled_video.write(image.astype(np.uint8))\n","\n","    compiled_video.release()\n","\n","def video_to_keyframes(video_path, target_fps = 15):\n","    captured_video = cv2.VideoCapture(video_path)\n","    fps = round(captured_video.get(cv2.CAP_PROP_FPS))\n","    skip_size = round(fps/target_fps)\n","    curr_frame = 0\n","\n","    keyframes=[]\n","    while True:\n","        ret, frame = captured_video.read()\n","        if not ret:\n","            break\n","        elif curr_frame % skip_size == 0:\n","            keyframes.append(ToResizedPIL(torch.from_numpy(frame).permute(2,0,1)))\n","        curr_frame+=1\n","\n","    captured_video.release()\n","    return keyframes\n","\n","def upscale(path_to_vid, upscaler, store_vid_path, frame_interval=nFrames, fps=15, device=device):\n","    print(\"Upscaling your video now...\")\n","    input_vid_keyframes=video_to_keyframes(path_to_vid)\n","    output_vid_keyframes=[]\n","\n","    upscaler.to(device)\n","    upscaler.eval()\n","\n","    length=len(input_vid_keyframes)\n","    y = (frame_interval-1)//2\n","    width, height = input_vid_keyframes[0].size\n","\n","    for i in range(length):\n","        if (i+1)%5 == 0:\n","            print(f\"{((i+1)/length)*100:.2f}% done...\")\n","        if i<y:\n","            l=0\n","        elif i>=length-y:\n","            l=length-frame_interval\n","        else:\n","            l=i-y\n","\n","        if i>length-1-y:\n","            h=length\n","        elif i<=y:\n","            h=frame_interval\n","        else:\n","            h=i+y+1\n","        #print(\"Started getting neighbours!\")\n","        neighbours = [input_vid_keyframes[j] for j in [j for j in range(l, h) if j!=i]]\n","        #print(\"Finished getting neighbours!\")\n","        #print(\"Started getting flows!\")\n","        flows = [get_flow(input_vid_keyframes[i], n, returnTensor=True, rescale=True).reshape(1,2,height,width).to(device) for n in neighbours]\n","        #print(\"Finished getting flows! Now reshaping and converting everything to tensors!\")\n","        inp = T(input_vid_keyframes[i]).reshape(1,3,height,width).to(device)\n","        neighbours = [T(n).reshape(1,3,height,width).to(device) for n in neighbours]\n","        #print(\"Finished reshaping!\")\n","        model.zero_grad()\n","        with torch.no_grad():\n","          output_vid_keyframes.append(upscaler(inp, neighbours, flows))\n","        #print(\"Finished computing flows. Appending to output_vid_keyframes done!\")\n","\n","    #print(\"Starting keyframes_to_video!\")\n","    keyframes_to_video([x.cpu() for x in output_vid_keyframes], store_vid_path, fps)\n","    print(f\"Finished upscaling. Saved to {store_vid_path}\")\n","\n","    upscaler.to(device)\n","    upscaler.train()\n","\n","    #Convert vid to keyframes\n","    #Rescale each keyframe to trained sizes\n","    #Store keyframes into a list\n","    #Consider an n-frame interval around the central frame\n","    #Compute the flow w.r.t central frame\n","    #Pass frame, neighbours and flows into model\n","    #Store frame into an output list\n","    #keyframes (from list) to video\n","    #store video at store_vid_path"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:39:16.986293Z","iopub.status.idle":"2024-03-10T13:39:16.986617Z","shell.execute_reply":"2024-03-10T13:39:16.986471Z","shell.execute_reply.started":"2024-03-10T13:39:16.986457Z"},"id":"OM-UgJLiYuAY","outputId":"1c2f334c-54cc-4160-f4e8-e062a2825b5e","trusted":true},"outputs":[],"source":["#upscale(\"Video.mp4\",model, \"video_gen.avi\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:39:16.988295Z","iopub.status.idle":"2024-03-10T13:39:16.988619Z","shell.execute_reply":"2024-03-10T13:39:16.988472Z","shell.execute_reply.started":"2024-03-10T13:39:16.988458Z"},"id":"I0tn9LZgWBDO","trusted":true},"outputs":[],"source":["# x = [x for x in range(150)]\n","# y = (nFrames-1)//2\n","# for i in range(len(x)):\n","#     if i<y:\n","#         l=0\n","#     elif i>=len(x)-y:\n","#         l=len(x)-nFrames\n","#     else:\n","#         l=i-y\n","\n","#     if i>len(x)-1-y:\n","#         h=len(x)\n","#     elif i<=y:\n","#         h=nFrames\n","#     else:\n","#         h=i+y+1\n","#     print(f\"i:{i}, l:{l}, h:{h}\")\n","#     print([x[j] for j in range(l, h) if j!=i])\n","#     print(\"\")"]},{"cell_type":"markdown","metadata":{"id":"pMsPxcJXWBDO"},"source":["Statistics:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-10T13:39:16.990084Z","iopub.status.idle":"2024-03-10T13:39:16.990385Z","shell.execute_reply":"2024-03-10T13:39:16.990249Z","shell.execute_reply.started":"2024-03-10T13:39:16.990237Z"},"id":"A1ymkCkkWBDP","trusted":true},"outputs":[],"source":["fig, axs = plt.subplots(1,2)\n","\n","axs[0].scatter([x for x in range(len(epoch_loss_hist))], [x.detach().cpu() for x in epoch_loss_hist])\n","axs[0].set_title(\"Epoch losses\")\n","\n","axs[1].plot([x for x in range(len(iter_loss_hist))], [x.detach().cpu() for x in iter_loss_hist])\n","axs[1].set_title(f\"Every {record} iteration losses\")\n","\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4561498,"sourceId":7826858,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":4}
